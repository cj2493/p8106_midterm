---
title: "P8106_midterm"
author: "Courtney Johnson & Jaisal Amin"
date: "March 25, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(mgcv)
library(patchwork)
library(pdp)
library(earth)
```

## Data

clean and also only keep variables that are a numerical value (only covered numerical modeling so far)
```{r, import}
master = read_csv("./master.csv") %>%
  janitor::clean_names() %>%
  mutate(sex = factor(sex, levels = c("male", "female")),
         age = factor(age, levels = c("5-14 years", "15-24 years", "25-34 years", 
                                      "35-54 years", "55-74 years", "75+ years"))) %>%
  rename(prominent_gen = generation) %>%
  select(suicides_100k_pop, everything())

master_num = master %>%
  select(suicides_100k_pop, year, sex, age, population, gdp_for_year, gdp_per_capita)  %>%
  mutate(sex = as.numeric(sex),
         age = as.numeric(age))
```
Here we have changed the age variable to a numerical to enable easier numerical analysis, but remember that the categories are: 1: 5-14, 2: 15-24, 3: 25-34 , 4: 35-54, 5: 55-74, 6: 75+

Also we did not include hdi because there were NAs
## Create x and y matrices for modeling
```{r, model_matrices}
ctrl1 = trainControl(method = "cv", number = 10)

set.seed(1)

sample = sample.int(n = nrow(master_num), size = floor(0.75*nrow(master_num)), replace = F)

x = model.matrix(suicides_100k_pop ~., master_num)[,-1]
y = master_num$suicides_100k_pop

train = master_num[sample,]
test = master_num[-sample,]

x_train = model.matrix(suicides_100k_pop~., train)[,-1]
y_train = train$suicides_100k_pop

x_test = model.matrix(suicides_100k_pop~., test)[,-1]
y_test = test$suicides_100k_pop
```

## Exploratory analysis and visualization
```{r, eda}
featurePlot(x, y, plot = "scatter", labels = c("","Y"),
            type = c("p"), layout = c(3, 2), alpha = 0.5)
hist(master_num$suicides_100k_pop)
qqnorm(master_num$suicides_100k_pop)
qqline(master_num$suicides_100k_pop)
cor(master_num)
pairs(master_num)
```

## Fit a linear model
```{r, lm}
set.seed(1)

lm.fit = train(x_train, y_train,
               method = "lm",
               trControl = ctrl1)

predy.lm = predict(lm.fit$finalModel, newdata = data.frame(x_test))
mean((predy.lm - y_test)^2)
```
test MSE = 285.0895

## Fit Ridge Regression
```{r, ridge, cache=TRUE}
set.seed(1)

ridge.fit = train(x_train, y_train,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = 0,
                                         lambda = exp(seq(-1, 10, length = 100))),
                  trControl = ctrl1)

predy.ridge = predict(ridge.fit$finalModel, newx = x_test, s = ridge.fit$bestTune$lambda, type = "response")
mean((predy.ridge - y_test)^2)
```
test error is 285.352

## Fit Lasso Regression
```{r, lasso, cache=TRUE}
set.seed(1)
lasso.fit = train(x_train, y_train,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = 1,
                                         lambda = exp(seq(-1, 5, length = 100))),
                  trControl = ctrl1)

predy.lasso = predict(lasso.fit$finalModel, newx = x_test, s = lasso.fit$bestTune$lambda, type = "response")

coef.lasso = predict(lasso.fit$finalModel, newx = x_test, s = lasso.fit$bestTune$lambda, type = "coefficients")

coef.lasso

mean((predy.lasso - y_test)^2)
```
test error 285.7102, 4 nonzero coefficients and one intercept

## Fit PCR
```{r, pcr, cache=TRUE}
set.seed(1)
pcr.fit = train(x, y,
                  method = "pcr",
                  tuneLength = 5,
                  trControl = ctrl1,
                  scale = TRUE)

predy.pcr = predict(pcr.fit$finalModel, newdata = x_test, ncomp = pcr.fit$bestTune$ncomp)

mean((predy.pcr - y_test)^2)
```
285.2069


## Fit GAM
```{r, gam, cache=TRUE}
set.seed(1)
gam.m1 = gam(suicides_100k_pop ~ year + sex + age + population + gdp_for_year + gdp_per_capita, data = master_num)
gam.m2 = gam(suicides_100k_pop ~ year + sex + age + population + s(gdp_for_year) + gdp_per_capita, data = master_num)

anova(gam.m1, gam.m2, test = "F")
plot(gam.m2)

gam.fit = train(x_train, y_train,
                method = "gam",
                tuneGrid = data.frame(method = "GCV.Cp", select = c(TRUE, FALSE)),
                trControl = ctrl1)
gam.fit$bestTune
gam.fit$finalModel

predy.gam = predict(gam.fit, newdata = data.frame(x_test))
mean((predy.gam - y_test)^2)
```
278.8389

## KNN
```{r, knn,warning=FALSE, message=FALSE, cache=TRUE}
set.seed(1)

knn.fit = train(x = train[, 2:7],
                   y = train$suicides_100k_pop,
                   method = "knn",
                   preProcess = c("center", "scale"),
                   tuneGrid = data.frame(k = seq(1, 200, by = 5)),
                   trControl = ctrl1)
#ggplot(knn.fit)

knn_pred = predict(knn.fit, newdata = data.frame(x_test))
mean((knn_pred - y_test)^2)
```
218.36

## MARS
```{r, mars, warning=FALSE, cache=TRUE}
mars_grid = expand.grid(degree = 1:2,nprune = 2:10)

set.seed(1)
mars.fit = train(x = train[, 2:7], 
                 y = train$suicides_100k_pop,
                 method = "earth",
                 tuneGrid = mars_grid,
                 trControl = ctrl1)

#ggplot(mars.fit)

mars_pred = predict(mars.fit, newdata = data.frame(x_test))
mean((mars_pred - y_test)^2)
```
256.40


## Regression Trees
```{r, tree}

```

## Support Vector Machines
```{r, svm}

```


## Boxplot
```{r, boxplot}
resamp = resamples(list(linear = lm.fit,
                        ridge = ridge.fit,
                        lasso = lasso.fit,
                        pcr = pcr.fit,
                        gam = gam.fit,
                        knn = knn.fit,
                        mars = mars.fit))

bwplot(resamp, metric = "RMSE")
```