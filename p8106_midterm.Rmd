---
title: "P8106_midterm"
author: "Courtney Johnson & Jaisal Amin"
date: "March 25, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(mgcv)
library(patchwork)
library(rpart)
library(rpart.plot)
library(party)
library(randomForest)
library(ranger)
library(e1071)
```

## Data

clean and also only keep variables that are a numerical value (only covered numerical modeling so far)
```{r, import}
master = read_csv("./master.csv") %>%
  janitor::clean_names() %>%
  mutate(sex = factor(sex, levels = c("male", "female")),
         age = factor(age, levels = c("5-14 years", "15-24 years", "25-34 years", 
                                      "35-54 years", "55-74 years", "75+ years"))) %>%
  rename(prominent_gen = generation) %>%
  select(suicides_100k_pop, everything())

master_num = master %>%
  select(suicides_100k_pop, year, sex, age, population, gdp_for_year, gdp_per_capita)  %>%
  mutate(sex = as.numeric(sex),
         age = as.numeric(age))
```
Here we have changed the age variable to a numerical to enable easier numerical analysis, but remember that the categories are: 1: 5-14, 2: 15-24, 3: 25-34 , 4: 35-54, 5: 55-74, 6: 75+

Also we did not include hdi because there were NAs
## Create x and y matrices for modeling
```{r, model_matrices}
ctrl1 = trainControl(method = "cv", number = 10)

set.seed(1)

sample = sample.int(n = nrow(master_num), size = floor(0.75*nrow(master_num)), replace = F)

x = model.matrix(suicides_100k_pop ~., master_num)[,-1]
y = master_num$suicides_100k_pop

train = master_num[sample,]
test = master_num[-sample,]

x_train = model.matrix(suicides_100k_pop~., train)[,-1]
y_train = train$suicides_100k_pop

x_test = model.matrix(suicides_100k_pop~., test)[,-1]
y_test = test$suicides_100k_pop
```

## Exploratory analysis and visualization
```{r, eda}
featurePlot(x, y, plot = "scatter", labels = c("","Y"),
            type = c("p"), layout = c(3, 2), alpha = 0.5)
hist(master_num$suicides_100k_pop)
qqnorm(master_num$suicides_100k_pop)
qqline(master_num$suicides_100k_pop)
cor(master_num)
pairs(master_num)
```

## Fit a linear model
```{r, lm}
set.seed(1)

lm.fit = train(x_train, y_train,
               method = "lm",
               trControl = ctrl1)

predy.lm = predict(lm.fit$finalModel, newdata = data.frame(x_test))
mean((predy.lm - y_test)^2)
```
test MSE = 262.7318

## Fit Ridge Regression
```{r, ridge}
set.seed(1)

ridge.fit = train(x_train, y_train,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = 0,
                                         lambda = exp(seq(-1, 10, length = 100))),
                  trControl = ctrl1)

predy.ridge = predict(ridge.fit$finalModel, newx = x_test, s = ridge.fit$bestTune$lambda, type = "response")
mean((predy.ridge - y_test)^2)
```
test error is 263.001

## Fit Lasso Regression
```{r, lasso}
set.seed(1)
lasso.fit = train(x_train, y_train,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = 1,
                                         lambda = exp(seq(-1, 5, length = 100))),
                  trControl = ctrl1)

predy.lasso = predict(lasso.fit$finalModel, newx = x_test, s = lasso.fit$bestTune$lambda, type = "response")

coef.lasso = predict(lasso.fit$finalModel, newx = x_test, s = lasso.fit$bestTune$lambda, type = "coefficients")

coef.lasso

mean((predy.lasso - y_test)^2)
```
test error 263.4252, 4 nonzero coefficients and one intercept

## Fit PCR
```{r, pcr}
set.seed(1)
pcr.fit = train(x, y,
                  method = "pcr",
                  tuneLength = 5,
                  trControl = ctrl1,
                  scale = TRUE)

predy.pcr = predict(pcr.fit$finalModel, newdata = x_test, ncomp = pcr.fit$bestTune$ncomp)

mean((predy.pcr - y_test)^2)
```
262.8632


## Fit GAM
```{r, gam}
set.seed(1)
gam.m1 = gam(suicides_100k_pop ~ year + sex + age + population + gdp_for_year + gdp_per_capita, data = master_num)
gam.m2 = gam(suicides_100k_pop ~ year + sex + age + population + s(gdp_for_year) + gdp_per_capita, data = master_num)

anova(gam.m1, gam.m2, test = "F")
plot(gam.m2)

gam.fit = train(x_train, y_train,
                method = "gam",
                tuneGrid = data.frame(method = "GCV.Cp", select = c(TRUE, FALSE)),
                trControl = ctrl1)
gam.fit$bestTune
gam.fit$finalModel

predy.gam = predict(gam.fit, newdata = data.frame(x_test))
mean((predy.gam - y_test)^2)
```
258.2585

## KNN
```{r, knn}

```

## MARS
```{r, mars}

```

## Regression Trees
```{r, tree}
set.seed(1)

rpart.fit = train(suicides_100k_pop ~ .,
  train,
  method = "rpart",
  tuneGrid = data.frame(cp = exp(seq(-6, -2, length = 20))),
  trControl = ctrl1)
ggplot(rpart.fit, highlight = TRUE)
rpart.plot(rpart.fit$finalModel)

set.seed(1)
rpart2.fit = train(suicides_100k_pop~.,
                   train,
                   method = "rpart2",
                   tuneGrid = data.frame(maxdepth = 1:7),
                   trControl = ctrl1)
ggplot(rpart2.fit, highlight = TRUE)
rpart.plot(rpart2.fit$finalModel)

set.seed(1)
ctree.fit = train(suicides_100k_pop~.,
                  train,
                  method = "ctree",
                  tuneGrid = data.frame(mincriterion = 1 - exp(seq(-6, -2, length = 20))),
                  trControl = ctrl1)
ggplot(ctree.fit, highlight = TRUE)
plot(ctree.fit$finalModel)

rf.grid = expand.grid(mtry = 1:2,
                 splitrule = "variance",
                 min.node.size = 1:2)
set.seed(1)
rf.fit = train(suicides_100k_pop~.,
               train,
               method = "ranger",
               tuneGrid = rf.grid,
               trControl = ctrl1)

ggplot(rf.fit, highlight = TRUE)
```

## Support Vector Machines
```{r, svm}

```


## Boxplot
```{r, boxplot}
resamp = resamples(list(linear = lm.fit,
                        ridge = ridge.fit,
                        lasso = lasso.fit,
                        pcr = pcr.fit,
                        gam = gam.fit))

bwplot(resamp, metric = "RMSE")
```